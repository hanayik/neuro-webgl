{
  "paths": [
    {
      "type": "file",
      "value": "index.md"
    },
    {
      "type": "dir",
      "name": "dev",
      "children": [
        {
          "type": "file",
          "value": "dev/font.md"
        },
        {
          "type": "file",
          "value": "dev/webgl.md"
        }
      ]
    },
    {
      "type": "file",
      "value": "screenshots.md"
    }
  ],
  "contents": [
    {
      "path": "index.md",
      "url": "index.html",
      "content": "# Niivue Docs\n\nThese docs are automatically generated during each build and deployed using Github pages.\n\n## API documentation\n\n[API docs](./api-docs/Niivue.html)\n\n## Live demo\n\nThe live demo exists to show off Niivue in its current state. This is sort of like working with the garage door open. The `master` branch features and capabilities will be shown here. Sometimes demos may look odd when new features are being tested incrementally.   \n\n[live demo with test images](./live-demo/index.html)\n\n[Live demo: with brain and hippocampus mask](./live-demo/index.html?urls=brain.nii.gz,hippo.nii.gz)\n\n[Live demo: load a nifti image from OpenNeuro.org](./live-demo/index.html?urls=https://openneuro.org/crn/datasets/ds002328/snapshots/1.0.0/files/sub-01:anat:sub-01_T1w.nii.gz) (may take a few seconds to load)\n",
      "html": "<h1 id=\"niivue-docs\">Niivue Docs <a class=\"heading-anchor-permalink\" href=\"#niivue-docs\">#</a></h1>\n<p>These docs are automatically generated during each build and deployed using Github pages.</p>\n<h2 id=\"api-documentation\">API documentation <a class=\"heading-anchor-permalink\" href=\"#api-documentation\">#</a></h2>\n<p><a href=\"./api-docs/Niivue.html\">API docs</a></p>\n<h2 id=\"live-demo\">Live demo <a class=\"heading-anchor-permalink\" href=\"#live-demo\">#</a></h2>\n<p>The live demo exists to show off Niivue in its current state. This is sort of like working with the garage door open. The <code>master</code> branch features and capabilities will be shown here. Sometimes demos may look odd when new features are being tested incrementally.</p>\n<p><a href=\"./live-demo/index.html\">live demo with test images</a></p>\n<p><a href=\"./live-demo/index.html?urls=brain.nii.gz,hippo.nii.gz\">Live demo: with brain and hippocampus mask</a></p>\n<p><a href=\"./live-demo/index.html?urls=https://openneuro.org/crn/datasets/ds002328/snapshots/1.0.0/files/sub-01:anat:sub-01_T1w.nii.gz\">Live demo: load a nifti image from OpenNeuro.org</a> (may take a few seconds to load)</p>\n",
      "id": 0
    },
    {
      "path": "dev/font.md",
      "url": "dev/font.html",
      "content": "## Introduction\n\nThere are [several approaches to render text in WebGL](https://stackoverflow.com/questions/25956272/better-quality-text-in-webgl). NiiVue uses \nViktor Chlumský's [multi-channel signed distance field](https://github.com/Chlumsky/msdfgen). The default font supplied with NiiVue is [Roboto](https://fonts.google.com/specimen/Roboto?preview.text_type=custom), created with [msdf-atlas-gen](https://github.com/Chlumsky/msdf-atlas-gen) using the command:\n\n```\nmsdf-atlas-gen.exe -font Roboto-Regular.ttf -charset chars.txt -pxrange 2 -dimensions 512 256 -format png -json fnt.json -imageout fnt.png\n```\n\nWhere chars.txt is a text file with the following characters\n\n```\n\"\\\"\\\\ ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz1234567890!`?'.,;:()[]{}<>|/@^$-%+=#_&~*\"\n```\nRunning the command will generate output\n\n```\nLoaded geometry of 95 out of 95 characters.\nGlyph size: 59.65625 pixels/EM\nAtlas image file saved.\nGlyph layout and metadata written into JSON file.\n```\n\nThe typeface or character set can be changed by modifiying the commands. NiiVue will read the JSON format created by msdf-atlas-gen (version 1.1), so to change the typeface used by NiiVue, simply replace the `fnt.json` and `fnt.png` files in `public` folder.\n\n## Usage\n\nViktor Chlumský's[GLSL fragment shader](https://github.com/Chlumsky/msdfgen) is easily adapted for WebGL. The uniform screenPxRange is described by Viktor as `the distance field range in output screen pixels. For example, if the pixel range was set to 2 when generating a 32x32 distance field, and it is used to draw a quad that is 72x72 pixels on the screen, it should return 4.5 (because 72/32 * 2 = 4.5).` Note that both distance range and distance field are reported in the JSON file created by msdf-atlas-gen: `\"distanceRange\":2,\"size\":59.65625`. Therefore, the complete WebGL fragment shader is:\n\n```\n#version 300 es\nprecision highp int;\nprecision highp float;\nuniform highp sampler2D fontTexture;\nuniform vec4 fontColor;\nuniform float screenPxRange;\nin vec2 vUV;\nout vec4 color;\nfloat median(float r, float g, float b) {\n    return max(min(r, g), min(max(r, g), b));\n}\nvoid main() {\n\tvec3 msd = texture(fontTexture, vUV).rgb;\n\t//color = vec4(msd, 1.0); return;\n    float sd = median(msd.r, msd.g, msd.b);\n    float screenPxDistance = screenPxRange*(sd - 0.5);\n    float opacity = clamp(screenPxDistance + 0.5, 0.0, 1.0);\n\tcolor = vec4(fontColor.rgb , opacity);\n}\n```\n",
      "html": "<h2 id=\"introduction\">Introduction <a class=\"heading-anchor-permalink\" href=\"#introduction\">#</a></h2>\n<p>There are <a href=\"https://stackoverflow.com/questions/25956272/better-quality-text-in-webgl\">several approaches to render text in WebGL</a>. NiiVue uses\nViktor Chlumský’s <a href=\"https://github.com/Chlumsky/msdfgen\">multi-channel signed distance field</a>. The default font supplied with NiiVue is <a href=\"https://fonts.google.com/specimen/Roboto?preview.text_type=custom\">Roboto</a>, created with <a href=\"https://github.com/Chlumsky/msdf-atlas-gen\">msdf-atlas-gen</a> using the command:</p>\n<pre><code>msdf-atlas-gen.exe -font Roboto-Regular.ttf -charset chars.txt -pxrange 2 -dimensions 512 256 -format png -json fnt.json -imageout fnt.png\n</code></pre>\n<p>Where chars.txt is a text file with the following characters</p>\n<pre><code>&quot;\\&quot;\\\\ ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz1234567890!`?'.,;:()[]{}&lt;&gt;|/@^$-%+=#_&amp;~*&quot;\n</code></pre>\n<p>Running the command will generate output</p>\n<pre><code>Loaded geometry of 95 out of 95 characters.\nGlyph size: 59.65625 pixels/EM\nAtlas image file saved.\nGlyph layout and metadata written into JSON file.\n</code></pre>\n<p>The typeface or character set can be changed by modifiying the commands. NiiVue will read the JSON format created by msdf-atlas-gen (version 1.1), so to change the typeface used by NiiVue, simply replace the <code>fnt.json</code> and <code>fnt.png</code> files in <code>public</code> folder.</p>\n<h2 id=\"usage\">Usage <a class=\"heading-anchor-permalink\" href=\"#usage\">#</a></h2>\n<p>Viktor Chlumský’s<a href=\"https://github.com/Chlumsky/msdfgen\">GLSL fragment shader</a> is easily adapted for WebGL. The uniform screenPxRange is described by Viktor as <code>the distance field range in output screen pixels. For example, if the pixel range was set to 2 when generating a 32x32 distance field, and it is used to draw a quad that is 72x72 pixels on the screen, it should return 4.5 (because 72/32 * 2 = 4.5).</code> Note that both distance range and distance field are reported in the JSON file created by msdf-atlas-gen: <code>&quot;distanceRange&quot;:2,&quot;size&quot;:59.65625</code>. Therefore, the complete WebGL fragment shader is:</p>\n<pre><code>#version 300 es\nprecision highp int;\nprecision highp float;\nuniform highp sampler2D fontTexture;\nuniform vec4 fontColor;\nuniform float screenPxRange;\nin vec2 vUV;\nout vec4 color;\nfloat median(float r, float g, float b) {\n    return max(min(r, g), min(max(r, g), b));\n}\nvoid main() {\n\tvec3 msd = texture(fontTexture, vUV).rgb;\n\t//color = vec4(msd, 1.0); return;\n    float sd = median(msd.r, msd.g, msd.b);\n    float screenPxDistance = screenPxRange*(sd - 0.5);\n    float opacity = clamp(screenPxDistance + 0.5, 0.0, 1.0);\n\tcolor = vec4(fontColor.rgb , opacity);\n}\n</code></pre>\n",
      "id": 1
    },
    {
      "path": "dev/webgl.md",
      "url": "dev/webgl.html",
      "content": "## Introduction\n\nThis project requires WebGL2. This specification was [finalized in January 2017](https://en.wikipedia.org/wiki/WebGL). It is supported by the current Chrome and Firefox browsers, but users of Safari must enable this `experimental` feature. NiiVue exploits WebGL2 features that [are not available in WebGL1](https://webgl2fundamentals.org/webgl/lessons/webgl2-whats-new.html). Specifically, the images are represented using non-Power of two 3D textures. The shaders used by WebGL2 are written using the [OpenGL ES 3.0](https://en.wikipedia.org/wiki/OpenGL_ES)version of the [OpenGL Shading Language (GLSL)](https://en.wikipedia.org/wiki/OpenGL_Shading_Language).\n\nhttps://gamedev.stackexchange.com/questions/132262/how-to-use-texelfetch\nbut in OpenGL when specifying an integer vertex attribute you must use glVertexAttribIPointer, not glVertexAttribPointer; see\nFor glVertexAttribIPointer ... Values are always left as integer values\nvec2 copies of the ivec2\n\n##### Textures\n\nThe term Textures refers to bitmap images that are stored on the graphics card. The WebGL context can only have a limited number of textures active at one time (with the command `activeTexture` deterimining which textures are available). You can think of these active textures as slots that are available for the shaders to access. NiiVue consistently uses the same slots for specific textures. This means that each draw call does not need to explicitly set the active textures. Therefore, these slots should be considered reserved and not used for other functions.\n\n - TEXTURE0: Background volume. This 3D scalar bitmap stores the voxel intensities of the background image.\n - TEXTURE1: Active colormap. This 2D RGBA bitmap converts the scalar background voxel intensities to RGBA values (e.g. Grayscale, Viridis). Note that the background and each overlay can have a unique colormap, so the selectColormap() call should be used to select a specific map.\n - TEXTURE2: Overlay volumes. This 3D RGBA bitmap stores the blended values of all loaded overlays.\n - TEXTURE3: Font. This is a 2D bitmap that stores the [multi-channel signed distance field typeface](https://github.com/Chlumsky/msdfgen) \n - TEXTURE6: Temporary 3D texture: this is used for compute shaders to reorient volumes (e.g. reformat an image from ASR to LIP orientation).\n\n##### Color Schemes\n\nThe user can choose different colormaps for displaying dark to bright voxels. In addition to grayscale, one can choose from the [viridis color palettes](https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html) (Cividis, Inferno, Plasma and Viridis) which are designed to be both salient and compatible with the most common forms of colorblindness. Since WebGL2 does not support 1D textures, these are codes as 2D bitmap textures (sampler2D, with a width of 256 pixels and a height of one pixel). This explains the GLSL definition `sampler2D colormap`, with reading using `texture(colormap, vec2(f, 0.5))` (where f is a fraction from 0..1, and 0.5 indicates sampling in the vertical middle of the bitmap).\n\n##### Overlays\n\nOverlay images are resliced to match the resolution of the background image. Multiple overlays may be loaded, but they are all blended together to generate a single RGBA texture. This overcomes OpenGL limits on the number of active textures loaded, and improves the speed of rednering. \n\nJavaScript is slower than natively compiled programs for many computations,  [using WebGL can dramatically increase peformance providing near native performance](http://openglinsights.com/discovering.html#WebGLforOpenGLDevelopers). Therefore, while using the GPU instead of the CPU can accelerate performance regardless of language, the benefits are greater for JavaScript. Reslicing 3D volumes is compute intensive. , WebGL . Github commits on March 13th 2021 illustrate the difference for computing these on the CPU versus the GPU (subsequent releases removed the CPU code). Specifically, CPU-based reslicing is about an order of magnitude slower for JavaScript than the equivalent functions in the natively compiled [MRIcroGL](https://github.com/rordenlab/MRIcroGL12). Therefore, niivue uses WebGL for reslicing data. Fortunately, the GPU-based WebGL reslicing algorithm is also about an order of magnitude faster than the standard JavaScript code.\n\nThe reslicing algorithm uses the [R8UI, R16I, R16UI and R32F](https://www.khronos.org/registry/OpenGL-Refpages/es3.0/html/glTexStorage3D.xhtml) base formats for the [NIfTI](https://nifti.nimh.nih.gov/pub/dist/src/niftilib/nifti1.h) `DT_UINT8`, `DT_INT16`, `DT_UINT16`, and `DT_FLOAT32` datatypes. One limitation of WebGL is that these texture formats are not [filterable](https://webgl2fundamentals.org/webgl/lessons/webgl-data-textures.html), meaning that only [nearest](https://open.gl/textures) interpolation is available. Therefore, overlays may appear blocky if there is not a one-to-one correspondence between the background image and an overlay. On the other hand, this is often desired for thresholded statistical maps (where many voxels are artificially zeroed). Another quirk of WebGL is that three shader programs are requied to support unsigned integer, signed integer and floating point textures (using `usampler3D`, `isampler3D` and `sampler3D` respectively).\n\nThe algorithm of the reslicing shader is shown in the Figure below. Each texture is a unit normalized cube (with the voxels accessed in the range 0..1 in the X,Y,Z dimensions, regardless of the number of voxels in the column, row and slice dimension). A 4x4 matrix provides the [affine transformation](https://en.wikipedia.org/wiki/Transformation_matrix) mapping the overlay (red in figure below) to the background image (black in image below). Since WebGL works with 2D framebuffers, the shader is run for every 2D slice of the background image's 3D volume. Thefore, this compute shader requires only the standard WebGL2 functions, without requiring the nascent [compute extensions](https://www.khronos.org/registry/webgl/specs/latest/2.0-compute/).\n \n![alt tag](overlay.png)\n\n##### Links\n\n - [WebGL Insights](https://webglinsights.github.io/index.html) is free and a terrific resource.\n",
      "html": "<h2 id=\"introduction\">Introduction <a class=\"heading-anchor-permalink\" href=\"#introduction\">#</a></h2>\n<p>This project requires WebGL2. This specification was <a href=\"https://en.wikipedia.org/wiki/WebGL\">finalized in January 2017</a>. It is supported by the current Chrome and Firefox browsers, but users of Safari must enable this <code>experimental</code> feature. NiiVue exploits WebGL2 features that <a href=\"https://webgl2fundamentals.org/webgl/lessons/webgl2-whats-new.html\">are not available in WebGL1</a>. Specifically, the images are represented using non-Power of two 3D textures. The shaders used by WebGL2 are written using the <a href=\"https://en.wikipedia.org/wiki/OpenGL_ES\">OpenGL ES 3.0</a>version of the <a href=\"https://en.wikipedia.org/wiki/OpenGL_Shading_Language\">OpenGL Shading Language (GLSL)</a>.</p>\n<p><a href=\"https://gamedev.stackexchange.com/questions/132262/how-to-use-texelfetch\">https://gamedev.stackexchange.com/questions/132262/how-to-use-texelfetch</a>\nbut in OpenGL when specifying an integer vertex attribute you must use glVertexAttribIPointer, not glVertexAttribPointer; see\nFor glVertexAttribIPointer … Values are always left as integer values\nvec2 copies of the ivec2</p>\n<h5 id=\"textures\">Textures <a class=\"heading-anchor-permalink\" href=\"#textures\">#</a></h5>\n<p>The term Textures refers to bitmap images that are stored on the graphics card. The WebGL context can only have a limited number of textures active at one time (with the command <code>activeTexture</code> deterimining which textures are available). You can think of these active textures as slots that are available for the shaders to access. NiiVue consistently uses the same slots for specific textures. This means that each draw call does not need to explicitly set the active textures. Therefore, these slots should be considered reserved and not used for other functions.</p>\n<ul>\n<li>TEXTURE0: Background volume. This 3D scalar bitmap stores the voxel intensities of the background image.</li>\n<li>TEXTURE1: Active colormap. This 2D RGBA bitmap converts the scalar background voxel intensities to RGBA values (e.g. Grayscale, Viridis). Note that the background and each overlay can have a unique colormap, so the selectColormap() call should be used to select a specific map.</li>\n<li>TEXTURE2: Overlay volumes. This 3D RGBA bitmap stores the blended values of all loaded overlays.</li>\n<li>TEXTURE3: Font. This is a 2D bitmap that stores the <a href=\"https://github.com/Chlumsky/msdfgen\">multi-channel signed distance field typeface</a></li>\n<li>TEXTURE6: Temporary 3D texture: this is used for compute shaders to reorient volumes (e.g. reformat an image from ASR to LIP orientation).</li>\n</ul>\n<h5 id=\"color-schemes\">Color Schemes <a class=\"heading-anchor-permalink\" href=\"#color-schemes\">#</a></h5>\n<p>The user can choose different colormaps for displaying dark to bright voxels. In addition to grayscale, one can choose from the <a href=\"https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html\">viridis color palettes</a> (Cividis, Inferno, Plasma and Viridis) which are designed to be both salient and compatible with the most common forms of colorblindness. Since WebGL2 does not support 1D textures, these are codes as 2D bitmap textures (sampler2D, with a width of 256 pixels and a height of one pixel). This explains the GLSL definition <code>sampler2D colormap</code>, with reading using <code>texture(colormap, vec2(f, 0.5))</code> (where f is a fraction from 0…1, and 0.5 indicates sampling in the vertical middle of the bitmap).</p>\n<h5 id=\"overlays\">Overlays <a class=\"heading-anchor-permalink\" href=\"#overlays\">#</a></h5>\n<p>Overlay images are resliced to match the resolution of the background image. Multiple overlays may be loaded, but they are all blended together to generate a single RGBA texture. This overcomes OpenGL limits on the number of active textures loaded, and improves the speed of rednering.</p>\n<p>JavaScript is slower than natively compiled programs for many computations,  <a href=\"http://openglinsights.com/discovering.html#WebGLforOpenGLDevelopers\">using WebGL can dramatically increase peformance providing near native performance</a>. Therefore, while using the GPU instead of the CPU can accelerate performance regardless of language, the benefits are greater for JavaScript. Reslicing 3D volumes is compute intensive. , WebGL . Github commits on March 13th 2021 illustrate the difference for computing these on the CPU versus the GPU (subsequent releases removed the CPU code). Specifically, CPU-based reslicing is about an order of magnitude slower for JavaScript than the equivalent functions in the natively compiled <a href=\"https://github.com/rordenlab/MRIcroGL12\">MRIcroGL</a>. Therefore, niivue uses WebGL for reslicing data. Fortunately, the GPU-based WebGL reslicing algorithm is also about an order of magnitude faster than the standard JavaScript code.</p>\n<p>The reslicing algorithm uses the <a href=\"https://www.khronos.org/registry/OpenGL-Refpages/es3.0/html/glTexStorage3D.xhtml\">R8UI, R16I, R16UI and R32F</a> base formats for the <a href=\"https://nifti.nimh.nih.gov/pub/dist/src/niftilib/nifti1.h\">NIfTI</a> <code>DT_UINT8</code>, <code>DT_INT16</code>, <code>DT_UINT16</code>, and <code>DT_FLOAT32</code> datatypes. One limitation of WebGL is that these texture formats are not <a href=\"https://webgl2fundamentals.org/webgl/lessons/webgl-data-textures.html\">filterable</a>, meaning that only <a href=\"https://open.gl/textures\">nearest</a> interpolation is available. Therefore, overlays may appear blocky if there is not a one-to-one correspondence between the background image and an overlay. On the other hand, this is often desired for thresholded statistical maps (where many voxels are artificially zeroed). Another quirk of WebGL is that three shader programs are requied to support unsigned integer, signed integer and floating point textures (using <code>usampler3D</code>, <code>isampler3D</code> and <code>sampler3D</code> respectively).</p>\n<p>The algorithm of the reslicing shader is shown in the Figure below. Each texture is a unit normalized cube (with the voxels accessed in the range 0…1 in the X,Y,Z dimensions, regardless of the number of voxels in the column, row and slice dimension). A 4x4 matrix provides the <a href=\"https://en.wikipedia.org/wiki/Transformation_matrix\">affine transformation</a> mapping the overlay (red in figure below) to the background image (black in image below). Since WebGL works with 2D framebuffers, the shader is run for every 2D slice of the background image’s 3D volume. Thefore, this compute shader requires only the standard WebGL2 functions, without requiring the nascent <a href=\"https://www.khronos.org/registry/webgl/specs/latest/2.0-compute/\">compute extensions</a>.</p>\n<p><img src=\"overlay.png\" alt=\"alt tag\"></p>\n<h5 id=\"links\">Links <a class=\"heading-anchor-permalink\" href=\"#links\">#</a></h5>\n<ul>\n<li><a href=\"https://webglinsights.github.io/index.html\">WebGL Insights</a> is free and a terrific resource.</li>\n</ul>\n",
      "id": 2
    },
    {
      "path": "screenshots.md",
      "url": "screenshots.html",
      "content": "# Niivue Example screenshots\n\n### Desktop Browsers\n\n#### Chrome\n\n![example image1](./screenshots/desktop-chrome1.png)\n\n![example image2](./screenshots/desktop-chrome2.png)\n\n### Mobile Browsers\n\n#### Android\n\n![example image3](./screenshots/mobile-android-chrome1.jpg)\n\n![example image4](./screenshots/mobile-android-chrome2.jpg)\n\n#### iOS\n\n![example image5](./screenshots/mobile-ios-safari-experimental1.png)\n\n![example image5](./screenshots/mobile-ios-safari-experimental2.png)\n\n![example image5](./screenshots/mobile-ios-safari-experimental3.png)\n\n",
      "html": "<h1 id=\"niivue-example-screenshots\">Niivue Example screenshots <a class=\"heading-anchor-permalink\" href=\"#niivue-example-screenshots\">#</a></h1>\n<h3 id=\"desktop-browsers\">Desktop Browsers <a class=\"heading-anchor-permalink\" href=\"#desktop-browsers\">#</a></h3>\n<h4 id=\"chrome\">Chrome <a class=\"heading-anchor-permalink\" href=\"#chrome\">#</a></h4>\n<p><img src=\"./screenshots/desktop-chrome1.png\" alt=\"example image1\"></p>\n<p><img src=\"./screenshots/desktop-chrome2.png\" alt=\"example image2\"></p>\n<h3 id=\"mobile-browsers\">Mobile Browsers <a class=\"heading-anchor-permalink\" href=\"#mobile-browsers\">#</a></h3>\n<h4 id=\"android\">Android <a class=\"heading-anchor-permalink\" href=\"#android\">#</a></h4>\n<p><img src=\"./screenshots/mobile-android-chrome1.jpg\" alt=\"example image3\"></p>\n<p><img src=\"./screenshots/mobile-android-chrome2.jpg\" alt=\"example image4\"></p>\n<h4 id=\"ios\">iOS <a class=\"heading-anchor-permalink\" href=\"#ios\">#</a></h4>\n<p><img src=\"./screenshots/mobile-ios-safari-experimental1.png\" alt=\"example image5\"></p>\n<p><img src=\"./screenshots/mobile-ios-safari-experimental2.png\" alt=\"example image5\"></p>\n<p><img src=\"./screenshots/mobile-ios-safari-experimental3.png\" alt=\"example image5\"></p>\n",
      "id": 3
    }
  ]
}